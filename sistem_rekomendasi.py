# -*- coding: utf-8 -*-
"""SISTEM REKOMENDASI.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gh2SsoAis759e8QxltTz01vtcXF25EO6

## IMPORT LIBRARY
"""

import pandas as pd
import tensorflow as tf
import numpy as np
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.layers import Embedding 
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import seaborn as sns

"""Pembacaan data baik file rating dan file movie """

rating = pd.read_csv('/content/sample_data/ratings.csv')

movie = pd.read_csv('/content/sample_data/movies.csv')

"""### Data Understanding"""

rating.info()

movie.info()

rating.describe()

"""Pembuatan bar plot untuk melihat banyaknya data pengguna dan banyaknya data film"""

plt.bar(['user','movie'],[len(rating.userId.unique()),len(movie.movieId.unique())])

"""### Data Preparation

#### Normalisasi

melakukan normalisasi data ke 1 dikarenakan perkalian dot menghasilkan nilai dari 0 sampai 1
"""

rating['rating'] = (rating['rating'] - rating['rating'].min())/(rating['rating'].max()-rating['rating'].min())

"""melihat nilai min dan max apakah sudah berhasil dinormalisasi atau belum"""

rating.describe()

"""#### One Hot Encoding"""

movie = pd.concat([movie,movie.genres.str.get_dummies('|')],axis=1)

movie = movie[movie['(no genres listed)']==0]

"""#### Penggabungan Data"""

data = rating.merge(movie,on=['movieId'],how='left')

dataSiap = data[['userId','movieId','rating']]

dataSiap.info()

num_user = dataSiap['userId'].unique().max()

num_movie = dataSiap['movieId'].unique().max()

"""data userId dan movie Id dimulai dari 1 maka perlu dikurangi 1 sehingga data bermulai dari index 0 """

dataSiap['userId'] = dataSiap['userId']-1

dataSiap['movieId'] = dataSiap['movieId']-1

"""#### Split data"""

X_train,X_test,Y_train,Y_test = train_test_split(dataSiap[['userId','movieId']],dataSiap['rating'],test_size=0.1,random_state=42)

"""### Modelling"""

class RecommenderNet(tf.keras.Model):
 
  # Insialisasi fungsi
  def __init__(self, num_users, num_resto, embedding_size, **kwargs):
    super(RecommenderNet, self).__init__(**kwargs)
    self.num_users = num_users
    self.num_movie = num_movie
    self.embedding_size = embedding_size
    self.user_embedding = layers.Embedding( # layer embedding user
        num_users,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.user_bias = layers.Embedding(num_users, 1) 
    self.resto_embedding = layers.Embedding( 
        num_resto,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.resto_bias = layers.Embedding(num_resto, 1) 
 
  def call(self, inputs):
    user_vector = self.user_embedding(inputs[:,0]) 
    user_bias = self.user_bias(inputs[:, 0])
    resto_vector = self.resto_embedding(inputs[:, 1]) 
    resto_bias = self.resto_bias(inputs[:, 1])
 
    dot_user_resto = tf.tensordot(user_vector, resto_vector, 2) 
 
    x = dot_user_resto + user_bias + resto_bias
    
    return tf.nn.sigmoid(x)

model = RecommenderNet(num_user, num_movie, 50)
 
# model compile
model.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer = keras.optimizers.Adam(learning_rate=0.001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

history = model.fit(
    x = X_train,
    y = Y_train,
    batch_size = 80,
    epochs = 50,
    validation_data = (X_test, Y_test)
)

"""### Evaluation

#### Loss
"""

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model_loss')
plt.ylabel('Loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

"""#### plot Metric"""

plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('model_metric')
plt.ylabel('RSME')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

"""### Result

memilih user secara random
"""

selectedUser = np.random.randint(0,num_user)

selectedUserDf = X_train.copy()

selectedUserDf = selectedUserDf[selectedUserDf['userId']==selectedUser]

"""melihat film yang telah ditonton oleh pengguna yang terpilih"""

selectedUserDf

movieIdforSelectedUser = selectedUserDf['movieId'].unique()

movieIdforSelectedUser

idMovieAll = dataSiap.movieId.unique()

idMovieAll.shape

"""membuang film yang telah ditonton pengguna sehingga yang akan dipredict hanya yang belum"""

tesMovie = list(set(idMovieAll) - set(movieIdforSelectedUser))

testMatrix = [[selectedUser,i] for i in tesMovie]

testMatrix = np.array(testMatrix)

testPredict = model.predict(testMatrix)

testPredict

testMatrix

"""mengurutkan nilai prediksi dari yang terbesar, dikarenakan nilai terbesar adalah nilai yang memungkinkan untuk film yang akan disukai pengguna"""

testPredict = np.argsort(np.squeeze(testPredict))[::-1]

movieRecom = testMatrix[testPredict[:10]]

movieLike = dataSiap[dataSiap['userId']==533].sort_values(by='rating',ascending=False).head(10)['movieId']

print("TOP 10 dengan rating tertinggi")
for i in movieLike:
  print('{}'.format(movie[movie['movieId']==i+1]['title'].values[0]))

print("TOP 10 of reccomendation movie")
for i in movieRecom:
  print('{}'.format(movie[movie['movieId']==i[1]+1]['title'].values[0]))